# Scikit-Learn 실습 05: 타이타닉 생존자 예측

이 문서는 타이타닉 탑승객 데이터를 분석하고, 생존자를 예측하는 머신러닝 파이프라인을 다룹니다. 데이터 전처리, 시각화, 모델 학습(Decision Tree, Random Forest, Logistic Regression), 교차 검증, 그리고 하이퍼 파라미터 튜닝(GridSearchCV)까지의 전체 과정을 포함합니다.

## 1. 핵심 내용 요약

1.  **데이터 전처리 (Preprocessing)**: `transform_features` 함수를 통해 결측치 처리(`Age`, `Cabin`, `Embarked`), 불필요한 피처 제거, 그리고 레이블 인코딩을 수행하여 머신러닝 알고리즘이 학습할 수 있는 형태로 데이터를 정제합니다.
2.  **다양한 모델 비교 학습**: `DecisionTreeClassifier`, `RandomForestClassifier`, `LogisticRegression` 세 가지 대표적인 분류 모델을 학습시키고 정확도를 비교합니다.
3.  **교차 검증 (Cross Validation)**: K-Fold 방식과 `cross_val_score` API를 사용하여 데이터 편향을 방지하고 모델의 일반화 성능을 검증합니다.
4.  **하이퍼 파라미터 튜닝 (GridSearchCV)**: `GridSearchCV`를 사용하여 의사결정 트리의 최적 파라미터(`max_depth`, `min_samples_split` 등)를 찾아내고 예측 성능을 극대화합니다.

---

## 2. 상세 코드 분석

### 2.1 라이브러리 임포트 및 데이터 로드

필요한 Scikit-learn 모듈과 데이터 분석 라이브러리를 임포트하고, CSV 파일을 로드합니다.

```python
from sklearn import preprocessing # 데이터 전처리를 위한 모듈 모음 (인코딩, 스케일링 등)
from sklearn.preprocessing import LabelEncoder # 범주형 문자 데이터를 숫자형으로 변환해주는 클래스
from sklearn.model_selection import train_test_split # 전체 데이터를 학습용과 테스트용으로 나누는 함수
from sklearn.tree import DecisionTreeClassifier # 의사결정 트리 알고리즘을 구현한 분류 모델
from sklearn.ensemble import RandomForestClassifier # 앙상블 기법 중 하나인 랜덤 포레스트 분류 모델
from sklearn.linear_model import LogisticRegression # 분류를 위한 선형 모델인 로지스틱 회귀 모델
from sklearn.metrics import accuracy_score # 모델 성능 평가를 위한 정확도 계산 함수
from sklearn.model_selection import KFold # K-Fold 교차 검증을 수행하기 위한 객체
from sklearn.model_selection import cross_val_score # 교차 검증을 더 간편하게 수행할 수 있는 함수
from sklearn.model_selection import GridSearchCV # 하이퍼 파라미터 튜닝과 교차 검증을 동시에 수행하는 객체

import numpy as np # 선형대수 및 배열 연산을 위한 핵심 라이브러리
import pandas as pd # 표(DataFrame) 형태의 데이터 처리를 위한 강력한 라이브러리
import matplotlib.pyplot as plt # 데이터 시각화(그래프)를 위한 기본 라이브러리
import seaborn as sns # matplotlib을 기반으로 더 예쁘고 통계적인 차트를 그리기 위한 라이브러리
import os # 파일 경로 조작 등 운영체제 기능을 사용하기 위한 모듈

# 현재 실행 중인 파이썬 스크립트 파일의 디렉토리 절대 경로를 구합니다.
path = os.path.dirname(__file__)

# 현재 스크립트 파일이 있는 경로와 CSV 파일명을 합쳐서 전체 경로를 만듭니다.
load_file = os.path.join(path, 'titanic_train.csv')

# pandas의 read_csv 함수를 이용해 CSV 파일을 DataFrame으로 불러옵니다.
titanic_df = pd.read_csv(load_file)

# 데이터가 잘 불러와졌는지 확인하기 위해 상위 3개 행만 출력합니다.
print(titanic_df.head(3))
```

### 2.2 데이터 정보 확인 및 결측치 처리

데이터의 구조를 확인하고, `NA` 값을 평균값이나 대체 문자('N')로 채웁니다.

```python
# 데이터의 전반적인 정보를 확인합니다.
print(f'\n ### train 데이터 정보 ### \n')
print(titanic_df.info())

# --- 결측치(NaN) 처리 시작 ---
# Age 컬럼의 NaN 값을 해당 컬럼의 평균값(mean)으로 채웁니다.
titanic_df['Age'].fillna(titanic_df['Age'].mean(), inplace=True)

# Cabin(선실) 컬럼의 NaN 값을 'N'이라는 문자로 채웁니다.
titanic_df['Cabin'].fillna('N', inplace=True)

# Embarked(정박 항구) 컬럼의 NaN 값을 'N'이라는 문자로 채웁니다.
titanic_df['Embarked'].fillna('N', inplace=True)

# 모든 결측치가 제거되었는지 확인합니다.
print(f'데이터 세트 NULL 값 개수: {titanic_df.isnull().sum().sum()}')
```

### 2.3 데이터 탐색 및 시각화

범주형 데이터의 분포를 확인하고, 성별, 선실 등급, 나이에 따른 생존율을 시각화합니다.

```python
# 범주형 데이터의 값 분포 확인
print(f'Sex 값 분포: \n{titanic_df["Sex"].value_counts()}')
print(f'\n Cabin 값 분포: \n{titanic_df["Cabin"].value_counts()}')
print(f'\n Embarked 값 분포: \n{titanic_df["Embarked"].value_counts()}')

# Cabin(선실) 첫 글자만 추출 (등급 구분)
titanic_df['Cabin'] = titanic_df['Cabin'].str[:1]
print(titanic_df["Cabin"].head(3))

# --- 데이터 시각화 및 분석 ---
# 성별(Sex)에 따른 생존율 시각화
sns.barplot(x='Sex', y='Survived', data=titanic_df)
plt.show()

# 객실 등급(Pclass)별 생존 확률 시각화
sns.barplot(x='Pclass', y='Survived', hue='Sex', data=titanic_df)
plt.show()

# 나이(Age) 카테고리화 함수
def get_category(age):
    cat = ''
    if age <= -1: cat = 'Unknown'
    elif age <= 5: cat = 'Baby'
    elif age <= 12: cat = 'Child'
    elif age <= 18: cat = 'Teenager'
    elif age <= 25: cat = 'Student'
    elif age <= 35: cat = 'Young Adult'
    elif age <= 60: cat = 'Adult'
    else : cat = 'Elderly'
    return cat

# 나이대별 생존율 시각화
plt.figure(figsize=(10, 6))
group_names = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Elderly']
titanic_df['Age_cat'] = titanic_df['Age'].apply(lambda x: get_category(x))
sns.barplot(x='Age_cat', y='Survived', hue='Sex', data=titanic_df, order=group_names)
plt.show()

# 임시 컬럼 삭제
titanic_df.drop(columns=['Age_cat'], axis=1, inplace=True)
```

### 2.4 전처리 함수 파이프라인 정의

반복적인 전처리 과정을 함수로 정의하여 편의성과 재사용성을 높입니다.

```python
# 범주형 피처 레이블 인코딩
def encode_features(dataDF):
    features = ['Cabin', 'Sex', 'Embarked']
    for feature in features:
        le = preprocessing.LabelEncoder()
        le = le.fit(dataDF[feature])
        dataDF[feature] = le.transform(dataDF[feature])
    return dataDF

# 결측치 처리
def fillna(df):
    df['Age'].fillna(df['Age'].mean(), inplace=True)
    df['Cabin'].fillna('N', inplace=True)
    df['Embarked'].fillna('N', inplace=True)
    df['Fare'].fillna(0, inplace=True)
    return df

# 불필요 피처 제거
def drop_features(dataDF):
    dataDF.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)
    return dataDF

# 포맷팅 및 인코딩 호출
def format_features(df):
    df['Cabin'] = df['Cabin'].str[:1]
    features = ['Cabin', 'Sex', 'Embarked']
    for feature in features:
        le = LabelEncoder()
        le = le.fit(df[feature])
        df[feature] = le.transform(df[feature])
    return df

# 전처리 마스터 함수
def transform_features(df):
    df = fillna(df)
    df = drop_features(df)
    df = format_features(df)
    return df
```

### 2.5 모델 학습 및 평가

데이터를 분리하고 3가지 모델을 학습시킨 후 정확도를 평가합니다.

```python
# 원본 데이터를 다시 로드하고 전처리 수행
titanic_df = pd.read_csv(load_file)
y_titanic_df = titanic_df['Survived']
X_titanic_df = titanic_df.drop('Survived', axis=1)
X_titanic_df = transform_features(X_titanic_df)

# 학습/테스트 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(X_titanic_df, y_titanic_df, test_size=0.2, random_state=11)

# 모델 객체 생성
dt_clf = DecisionTreeClassifier(random_state=11)
rf_clf = RandomForestClassifier(random_state=11)
lr_clf = LogisticRegression()

# 1. DecisionTreeClassifier
dt_clf.fit(X_train, y_train)
dt_pred = dt_clf.predict(X_test)
print(f'DecisionTreeClassifier 정확도: {accuracy_score(y_test, dt_pred):.4f}')

# 2. RandomForestClassifier
rf_clf.fit(X_train, y_train)
rf_pred = rf_clf.predict(X_test)
print(f'RandomForestClassifier 정확도: {accuracy_score(y_test, rf_pred):.4f}')

# 3. LogisticRegression
lr_clf.fit(X_train, y_train)
lr_pred = lr_clf.predict(X_test)
print(f'LogisticRegression 정확도: {accuracy_score(y_test, lr_pred):.4f}')
```

### 2.6 교차 검증 (Cross Validation)

K-Fold 방식과 `cross_val_score`를 사용하여 모델 검증을 수행합니다.

```python
# K-Fold 교차 검증 함수
def exec_kfold(clf, folds=5):
    kfold = KFold(n_splits=folds)
    scores = []

    for iter_count, (train_index, test_index) in enumerate(kfold.split(X_titanic_df)):
        X_train, X_test = X_titanic_df.values[train_index], X_titanic_df.values[test_index]
        y_train, y_test = y_titanic_df.values[train_index], y_titanic_df.values[test_index]

        clf.fit(X_train, y_train)
        pred = clf.predict(X_test)
        accuracy = accuracy_score(y_test, pred)
        scores.append(accuracy)
        print(f'교차 검증 {iter_count+1} 정확도: {accuracy:.4f}')

    print(f'평균 정확도: {np.mean(scores):.4f}')

exec_kfold(dt_clf, folds=5)

# cross_val_score 사용 (간편 교차 검증)
scores = cross_val_score(dt_clf, X_titanic_df, y_titanic_df, cv=5)
for iter_count, accuracy in enumerate(scores):
    print(f'교차 검증 {iter_count+1} 정확도: {accuracy:.4f}')
print(f'평균 정확도: {np.mean(scores):.4f}')
```

### 2.7 하이퍼 파라미터 튜닝 (GridSearchCV)

GridSearchCV를 이용해 Decision Tree의 최적 파라미터를 찾습니다.

```python
# 튜닝할 파라미터 정의
parameters = {
    'max_depth': [2, 3, 5, 10],
    'min_samples_split': [2, 3, 5],
    'min_samples_leaf': [1, 5, 8]
}

# GridSearchCV 객체 생성 및 수행
grid_dclf = GridSearchCV(dt_clf, param_grid=parameters, scoring='accuracy', cv=5)
grid_dclf.fit(X_train, y_train)

print(f'GridSearchCV 최적 하이퍼 파라미터: {grid_dclf.best_params_}')
print(f'GridSearchCV 최고 정확도: {grid_dclf.best_score_:.4f}')

# 최적 모델로 최종 평가
best_dclf = grid_dclf.best_estimator_
dpredictions = best_dclf.predict(X_test)
accuracy = accuracy_score(y_test, dpredictions)
print(f'테스트 세트에서의 DecisionTreeClassifier 정확도: {accuracy:.4f}')
```
